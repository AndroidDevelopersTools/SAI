# SAI-FGSM implementation
---
We reconsider the generation of adversarial examples as a search problem within the neighborhood of the real example. Inspired by the effectiveness of heuristic search methods in finding global optima, we propose the Simulated Annealing Iterative Fast Gradient Sign Method (SAI-FGSM), which leverages the principle of thermal annealing and the Metropolisâ€“Hastings acceptance criterion to escape local minima and saddle points, thereby significantly increasing the probability of discovering global optima.

<img width="1209" height="807" alt="image" src="https://github.com/user-attachments/assets/6c9a2d22-519e-4212-a327-c99862008f19" />


# Setting
We choose 1000 images belonging to the 1000 categories from ILSVRC 2012 validation set, which are almost correctly classified by all the testing models. 


All the images are resized to 224 $\times$ 224 and the number of iterations is fixed to 10. The temperature drops from 10 to 0.5 for all models and the perturbation $\varepsilon$ is set to 0.03. We compare our approach with MI, DI, TI, VMI, PAM, and two state-of-the-art methods BSR and VMI-CWA. We choose five normally trained models - AlexNet, VGG-16, ResNet-101, ShuffleNet-V2, MobileNet-V3 from TorchVision and two adversarially trained models - ResNet-50, XCiT-S12 from RobustBench. They contain both normal and robust models, which is effective to evaluate the transferability of algorithms to black-box models with attack success rate(ASR).


# Result
<img width="1171" height="1143" alt="image" src="https://github.com/user-attachments/assets/6466d0dc-71b2-4ec0-9919-ea9fba505150" />

<img width="1182" height="853" alt="image" src="https://github.com/user-attachments/assets/b19ebc57-776b-4477-91cd-7f62fd88683d" />



## Install
### Environment
Please create a new conda environment and run:
```bash
pip install requirements.txt
```


### Data
**CIFAR10** will be downloaded automatically    
For **PACS** dataset, please refer to ./data/PACS.py for install    
For **NIPS17** dataset, you can run \
```bash
kaggle datasets download -d google-brain/nips-2017-adversarial-learning-development-set
```
and then put it into ./resources/NIPS17

### Model Checkpoints
All the models in our paper are from torchvision, robustbench, timm library, and the checkpoints will be downloaded automatically.

We also encapsulate some models and defenses in *"./models"* and *"./defenses"*. If you want to attack them, you can download their checkpoints by yourself

---

## Usage

### Code Framework

> attacks: Some attack algorithms. Including VMI, VMI-CW, CW, SAM, etc.      
> data: loader of CIFAR, NIPS17, PACS    
> defenses: Some defenses algorithm    
> experiments: Example codes    
> models: Some pretrained models   
> optimizer: scheduler and optimizer   
> tester: some functions to test accuracy and attack success rate   
> utils: Utilities. Like draw landscape, get time, HRNet, etc.     


### Basic functions

```
tester.test_transfer_attack_acc(attacker:AdversarialInputBase, loader:DataLoader, target_models: List[nn.Module]) \
```

This function aims to get the attack success rate on loader against target models



```
attacker = xxxAttacker(train_models: List[nn.Module])
```
You can initialize attacker like this.

### Examples
Here is an example of testing attack success rate on NIPS17 loader.
```python
from models import resnet18, Wong2020Fast, Engstrom2019Robustness, BaseNormModel, Identity
from attacks import MI_CommonWeakness
attacker = MI_CommonWeakness([
    BaseNormModel(resnet18(pretrained=True)), # model that requires normalization
    Identity(Wong2020Fast(pretrained=True)) # model that do not need normalization
])

from tester import test_transfer_attack_acc
from data import get_NIPS17_loader
test_transfer_attack_acc(attacker, 
                         get_NIPS17_loader(), 
                         [
                             Identity(Wong2020Fast(pretrained=True)), # white box attack
                             Identity(Engstrom2019Robustness(pretrained=True)), # transfer attack
                          ]
                         )
```


For more example codes, please visit *'./experiments'* folder. There are some example codes using our framework to attack and draw landscapes. I believe you can quickly get familiar with our framework via these example codes.


### HRNet
HRNet is a function that aims to reduce memory cost when crafting adversarial examples.

**We haven't implemented the convolution of HRNet. Up to now, HRNet can only help to reduce about 30% of memory cost**

#### Usage
```python
from models import resnet18
from utils import change

model = resnet18()
model = change(model)
```


---

